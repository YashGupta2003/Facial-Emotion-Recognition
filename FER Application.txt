import cv2
import os
import tensorflow as tf
import numpy as np
import streamlit as st
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import warnings
from mtcnn import MTCNN  # Efficient face detection using MTCNN

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Model paths
model_dir = r'C:\Users\ADMIN\Documents\pp\pp'
model_name = 'Yashgupta.keras'
model_path = os.path.join(model_dir, model_name)

# Data paths
data_dir = r'C:\Users\ADMIN\Documents\pp\pp\collected_data'

# Load pre-trained model
def load_trained_model():
    try:
        if not os.path.exists(model_path):
            st.error(f"Model not found at {model_path}. Please ensure the model exists or train it first.")
            exit()
        model = load_model(model_path)
        model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
        return model
    except Exception as e:
        st.error(f"Error loading model: {e}")
        exit()

# Load MTCNN for face detection
detector = MTCNN()

# Initialize emotion classes and emojis
emotion_classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
emotion_emojis = {
    'angry': 'üò†',
    'disgust': 'ü§¢',
    'fear': 'üò®',
    'happy': 'üòä',
    'neutral': 'üòê',
    'sad': 'üòû',
    'surprise': 'üòÆ'
}

# Initialize emotion scores
emotion_scores = {
    'angry': 1,
    'disgust': 1,
    'fear': 2,
    'happy': 5,
    'neutral': 3,
    'sad': 1,
    'surprise': 4
}

# Preprocess face image for model prediction
def preprocess_face(face):
    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
    face = cv2.resize(face, (48, 48))
    face = img_to_array(face) / 255.0
    face = np.expand_dims(face, axis=0)
    return face

# Save new face data for retraining
def save_face_data(face, emotion_label):
    label_dir = os.path.join(data_dir, emotion_label)
    if not os.path.exists(label_dir):
        os.makedirs(label_dir)
    count = len(os.listdir(label_dir))
    file_path = os.path.join(label_dir, f"{count + 1}.png")
    cv2.imwrite(file_path, face)

# Retrain model with new data
def retrain_model():
    data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255.0, validation_split=0.2)
    train_data = data_gen.flow_from_directory(data_dir, target_size=(48, 48), batch_size=32, class_mode='categorical', subset='training')
    val_data = data_gen.flow_from_directory(data_dir, target_size=(48, 48), batch_size=32, class_mode='categorical', subset='validation')

    model.fit(train_data, validation_data=val_data, epochs=10, callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)])

    model.save(model_path)

# Webcam and emotion detection in real-time
def start_camera():
    cap = cv2.VideoCapture(0)
    stframe = st.empty()
    emotion_label_container = st.empty()
    emoji_container = st.empty()
    score_container = st.empty()

    frame_skip = 3  # Predict once every 3 frames
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            st.error("Failed to capture video. Please check your camera.")
            break

        faces = detector.detect_faces(frame)
        if len(faces) > 0:
            # Process the first detected face
            x, y, w, h = faces[0]['box']
            face = frame[y:y + h, x:x + w]
            face_preprocessed = preprocess_face(face)
            prediction = model.predict(face_preprocessed)
            class_idx = np.argmax(prediction)
            confidence = np.max(prediction)

            if frame_count % frame_skip == 0:
                emotion = emotion_classes[class_idx]
                emoji = emotion_emojis.get(emotion, 'üòê')
                score = emotion_scores.get(emotion, 0)

                # Update Streamlit UI
                emotion_label_container.markdown(f"**Detected Emotion:** {emotion}")
                emoji_container.markdown(f"**Emoji:** {emoji}")
                score_container.markdown(f"**Emotion Score:** {score}/5")

                save_face_data(face, emotion)

        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        stframe.image(frame, channels="RGB", use_container_width=True)

        frame_count += 1

        if st.session_state.get('stop', False):
            break

    cap.release()
    stframe.empty()

    retrain_model()

# Main function to control Streamlit app
def main():
    st.title('Facial Emotion Recognition by Yash Gupta')
    st.write("Press the button below to start or stop live emotion detection.")

    if 'stop' not in st.session_state:
        st.session_state['stop'] = False

    if st.button("Start Camera" if not st.session_state['stop'] else "Stop Camera"):
        st.session_state['stop'] = not st.session_state['stop']
        if not st.session_state['stop']:
            start_camera()

if __name__ == '__main__':
    model = load_trained_model()
    main()
